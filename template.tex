\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2018}
\usepackage{subfigure}

\title{AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale}
\name{Jiayu Du$^1$, Xuechen Liu$^1$, Hui Bu$^2$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$AISHELL foundation$^*$\thanks{* AISHELL foundation is a non-profit online organization, dedicated to pushing forward speech industry via open-sourcing database to research institutes and contributing codes to open-source speech community.} \\
  $^2$Beijing Shell Shell Technology Co. Ltd., Beijing, China}
\email{aishell.foundation@gmail.com, buhui@aishelldata.com}

\begin{document}

\maketitle
%
\begin{abstract}
AISHELL-1 is by far the largest open-source speech corpus available for Mandarin
speech recognition research. It was released with a baseline system containing
training and testing pipelines for Mandarin ASR. In AISHELL-2, 1000 hours of
clean read-speech data is published, which is free for academic usage. On top of
AISHELL-2 corpus, an improved recipe is developed and released, containing key
components for industrial applications, such as Chinese word segmentation,
robust data augmentation, domain-specific fine-tuning towards acoustic model and
language model, flexible vocabulary expension and phone set transformation,
etc. Pipelines supporting various state-of-the-art techniques are provided, such
as recurrent neural network acoustic models. Moreover, a real-time ASR demo is
released, to help AISHELL-2 users evaluate and experience target ASR system via
their own laptop microphone. For research community, we hope that AISHELL-2
corpus can be a solid resource for topics like transfer learning and robust
ASR. For industry, we hope AISHELL-2 recipe can be a helpful reference for
building meaningful industrial systems and products.
\end{abstract}
\noindent\textbf{Index Terms}: Speech recognition, Mandarin ASR, Industrial Speech Recognition

\section{Introduction}

Automatic Speech recognition (ASR) is one of the application domains in the bloom of Artificial
Intelligence (AI) which recently received huge attention. Huge effort has been made in both research and industry
to improve ASR system performance. Among all solutions, Deep Learning has been dominating
this field in the recent half decade. One advantage offered by it is that neural network (NN) models generally turn out to be more robust when
huge amount of data is provided. This is good since thanks to the emerging market of various smart devices with rising online network usage, real data from
real-time scenarios can sometimes be fetched consistently. Therefore, accessing and 
collecting data has become easier than before for software industry.
However, on the other hand, research community still normally has limited access to real-world
application data. Number of ASR solutions, as a result, mostly can not scale well and hence not applicable for industrial use. This leads to the need of high-quality corpus for research use while being easily deployable to industrial level, just like what happened in the field of computer vision (e.g. ImageNet~\cite{imagenet} and COCO~\cite{coco}). No such corpus in Mandarin ASR has been released until AISHELL-1~\cite{aishell1}.
%In the field of computer vision, there are many high quality free data sets which transform many research
%efforts into industrial applications, such as ImageNet~\cite{imagenet} and
%COCO~\cite{coco}. However, in the field of Mandarin ASR, there was no free
%corpus with sufficient amount of high quality speech data to build a meaningful
%research baseline, until the release of AISHELL-1~\cite{aishell1}.
It contains 170 hours of Mandarin speech with high quality human transcriptions. Various training and evaluation recipes based on such corpus have been released on Kaldi~\cite{kaldi}, which is a large and robust framework for speech research. To the best knowledge of the authors, it is the first fully open-sourced corpus for Mandarin ASR and
it has begun doing goods for researchers who need high quality Mandarin speech data(e.g. ~\cite{do2017, do2018_1, do2018_2}).

% Another issue with the Mandarin speech recognition research is that, there is
% still gap between a research prototype for demonstration and a working system
% for application. The open-source movement improves the transparency and
% availability of many research achievements. However, the attemp of direct
% deployment of a working system from an open-source demostration often fail. The
% effort of knowledage transfering from the research topic to application domain
% is enourmously high, and is normally off the record.

Furthering the success of AISHELL-1, in this rest of this paper, we officially announce and gently introduce AISHELL-2 with baseline systems developed as well as reference statistics for industrial-scale Mandarin ASR research.
%research. AISHELL-2 contains around 1000 hours of clean read-speech data, which is free
%for academic usage. On top of AISHELL-2 corpus, an improved speech recognition
%recipe is published. Firstly, it contains must-have components for Mandarin ASR,
%such as Chinese word segmentation, and a new design for customizable Chinese
%lexicon. Secondly, it presents some effective and straight forward approaches
%for large-scale ASR system tuning, such as robust data augmentation, acoustic
%channel fine-tuning, and domain adaptation via language model
%interpolation. Moreover, a real-time ASR demo is provided, to help not only
%AISHELL-2 users, but also other recipe developers, to try their ASR systems via
%their own laptop microphone.
%
%This paper is organized as below. 
Section 2 introduces the structure and specifications of the AISHELL-2 corpus. Section 3 describes the AISHELL-2
baseline recipe, including data preparation, GMM training, DNN training, and the
evaluations. Section 4 presents some effective procedures to transfer a large
scale ASR baseline to a target application.

\section{The AISHELL-2 corpus}

The AISHELL-2 corpus contains 1000 hours of clean speech. The raw data was recorded under a multi-channel environment with three devices - a high fidelity microphone, an Android smartphone and an iPhone. The relative position of speaker and devices are shown in Figure~\ref{fig:setup}. The data recorded by the iPhone is the one open-sourced. Speaker, environment and content coverage are fully considered and explained as below:
\begin{itemize}
\item There are 1991 speakers participated in the recording, including 845 male speakers and 1146 female speakers. Age of speaker ranges from 11 to over 40. Ideally the speakers shall speak everything to be recorded in Mandarin, while there are some slight accent variations. Generally speaking of the accents, there were 1293 speakers using Northern ones, 678 speakers using Southern ones and 20 speakers use other accents during recording. 
\item 1347 speakers got themselves recorded in a studio while the rest were in a living room with natural reverberation.
\item The content of the recording covers 8 major topics: voice commands such as IoT device control and digital sequential input, places of interest, entertainment, finance, technology, sports, English spellings and free speaking without specific topic. The total number of prompts is around half a million.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{setup.jpg}
  \caption{Recording setup}
  \label{fig:setup}
\end{figure}

\noindent Aside from the part of the AISHELL-2 corpus introduced above, which is supposed to be used for training, we also provide development and test sets. The development set contains 2500 utterances from 5 speakers and the test set contains 5000 utterances from 10 speakers respectively. Each speaker among the 15 contributed approximately half an hour of his/her own voice, covering 500 prompts. First 7 prompts of all speakers are sampled from a shared prompt pool extracted from high frequency online queries. The gender balance among the sets was concerned and properly covered as well.

\section{The AISHELL-2 recipe}

Based on AISHELL-2 corpus, a baseline Kaldi recipe is also released, including
data processing, robustness data augmentation, acoustic model and language model
training, test set
evaluation\footnote{https://github.com/aishell-foundation/aishell-2}. Although
mostly based on standard Kaldi recipe, AISHELL-2 recipe is different in several
ways.

\subsection{Lexicon and word segmentation}

Different from English system, Mandarin ASR systems require word
segmentation. In AISHELL-1, word segmentation was implemented via forward
maximum matching algorithm[Ref], based on a variation of open-source mandarin
dictionary CC-CEDIT\footnote{https://cc-cedict.org/wiki}. In AISHELL-2, an
open-source Chinese dictionary called DaCiDian is
released\footnote{https://github.com/aishell-foundation/DaCiDian}. In most
common Chinese dictionaries, words are directly mapped to phonemes. While in
DaCiDian, this mapping is decomposed into 2 independent layers. An examplar
DaCiDian structure is shown in Figure~\ref{fig:lex1} and Figure~\ref{fig:lex2}.

\begin{itemize}
\item The first layer maps word to PinYin syllables~\cite{pinyin}. Anyone who is
  familiar with PinYin (basically every Mandarin speaker), can enrich DaCiDian's
  vocabulary by adding new words into this layer.
\item The second layer is a mapping from Pinyin syllable to phoneme. ASR system
  developers can easily adapt DaCiDian to their own phone set by redefining this
  layer of mapping.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{dacidianl1.png}
  \caption{Layer 1 of DaCiDian}
  \label{fig:lex1}
\end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{dacidianl2.png}
  \caption{Layer 2 of DaCiDian}
  \label{fig:lex2}
\end{figure}

In terms of word segmentation, we choose a popular and easy-to-use open-source
toolkit called Jieba~\cite{jieba}, it implements a trie-tree based algorithm and
supports vocabulary customization.  Base on DaCiDian and Jieba, we provide a
script to segment AISHELL-2 acoustic data transcription and language model text.

\subsection{Audio augmentation}

Most commercial systems are designed to deal with spontaneous speech. However,
due to legal issues and user privacy protection, spontaneous speech data is not
always feasible or too expensive to collect. So we design AISHELL-2 as a clean
read-speech database. As expected, AISHELL-2 system is by nature not as robust
as those commercial systems powered by real-user speech data. To alleviate this
problem, we applied several perturbation approaches to enhance system
robustness.

\begin{itemize}
\item speech-rate perturbation

  In~\cite{tomko}, author compared two types of speech-rate perturbation: speed
  and tempo. They are equivalent in terms of tuning speech-rate. But in
  practice, improper speed perturbation parameters (large speed-up or slow-down
  effect) lead to frequency domain distortion. So we choose tempo perturbation,
  to adapt slow read-speech to fast spontaneous speech.

\item volume perturbation

  standard Kaldi recipe uses scale factor to control volume perturbation. For
  typical setup, all utterances are scaled by a random number between [0.125,
    2]. However in practice, we found that improper scale-range setup, collided
  with utterance with unexpected volume, leads to severe saturation or
  silence-like perturbation. In AISHELL-2, we propose and implement volume
  perturbation in a slightly different way. Instead of scale-factor controlling,
  we implemented target-volume controlling. We first probed the volume for each
  utterance, and scale the entire utterance with a random scale, so that its max
  sample value lies in a target range, such as [800, 32000].

\item noise robustness

  AISHELL-2 system is vulnerable to background noise because the entire corpus
  is recorded in silent environment. Although clean data is not directly
  suitable for commercial system, we can add specific noise on clean data to
  simulate real world speech data. Unfortunately, until the release of
  AISHELL-2, we are not able to collect a diverse noise library.

  Noise library might be our next plan, but at the moment, we hope
  \begin{itemize}
    \item For research community, AISHELL-2 database could be a helpful
      ingredient to explore more effective techniques for robust ASR research.
    \item For industry, we encourage developers to collect noise library of
      their own interests, clean AISHELL-2 data combined with these noise and
      simulation techniques, should be enough to breed a usable baseline
      product, then developers could open the gate to collect real world user
      data and iterate their products.
  \end{itemize}
\end{itemize}

\subsection{Acoustic model}

Two versions of the acoustic model training pipelines are provided, including
the GMM training based on maximum likelihood and the hybrid DNN-HMM training
based on cross entropy.

The GMM models are trained using 13 dimensional MFCCs. For sake of real time
demonstration, pitch is not included as acoustic feature. The training contains
four steps. A monophone model is trained to set a starting point for the
triphone models. A small triphone model and a larger triphone model are
consecutively trained using delta features. After that, a more sophisticated
feature transform method is applied to replace the delta features. Linear
discriminant analysis (LDA) is applied on a stack of frames to reduce the
dimension and MLLT based global transform is estimated iteratively. This is a
standard setup in Kaldi recipes. The resulting number of physical GMMs of the
four steps are 605, 3216, 5720 and 8080 respectively.

GMM training of the AISHELL-2 terminates at the speaker independent stage
without speaker dependent transform, such as fMLLR. From privacy point of view,
it is often impractical to use customers' personal data and account
infomation. From architecture point of view, deploying a large scale
speaker-dependent commercial system is complex and costly. Therefore, AISHELL-2
adopts a simplified speaker independent style for GMM training. Moreover,
spending time and computation in GMM stage is not very helpful for final
performance.

Neural network acoustic models are trained using 80 dimensional filter bank
features. A time-delay neural network (TDNN) with 4 time-delay layers and 2
full-connect layers is trained using batch normalization~\cite{tdnn}. All of the
layers use 850 output RELU neurons~\cite{relu}. A recurrent neural network with
3 LSTM layers is trained using chunked BPTT with a output delay factor of
5~\cite{lstm}. Each of the LSTM layers contains 1024 cells and a projection of
256 output neurons~\cite{lstmp}.

\subsection{Language model}

A trigram language model is trained on 5.7 million words of the training
scripts. Out-of-vocabulary (OOV) words are mapped into $<$UNK$>$. The language
model is trained using Kneser-Ney smoothing and the final model has 516552
unigrams, 1498603 bigrams and 932475 trigrams.

\subsection{Evaluations}

The presented baseline system is trained on a standalone server with ordinary
configurations. The CPU is Intel Xeon processor of 2.3GHz and the GPUs are four
Nvidia Titan Xp. Character Error Rate (CER) is used as evaluation metric because
it is widely adopted in Mandarin ASR community. The results along with the total
training time of each model are presented in Table~\ref{tab:base}. Note that
during training we only use iOS data for acoustic modeling, but we evaluate
these models on dev and test sets for all 3 channels(Android, Mic, and iOS). As
expected, system performance on iOS outperforms Android and Mic, due to better
acoustic channel matching between training and testing.

\begin{table*}[th]
  \caption{Baseline system results and training time}
  \label{tab:base}
  \centering
  \begin{tabular}{ llllllll }
    \toprule
    CER               &  dev\_android           &  dev\_ios           &  dev\_mic           & test\_android            &  test\_ios           &  test\_mic          &  Training time         \\
    \midrule
    Monopphone        &  55.10                 &  50.85             &  54.95             &  51.35                  &  50.14              &  50.03             &  0.5                   \\
    Small triphone    &  32.66                 &  27.70             &  31.47             &  30.33                  &  27.90              &  28.76             &  1                     \\
    Large triphone    &  30.71                 &  25.72             &  29.18             &  28.26                  &  26.12              &  26.81             &  2                     \\
    LDA+MLLT          &  26.32                 &  22.29             &  25.34             &  24.44                  &  22.48              &  23.16             &  2.3                   \\
    TDNN              &  15.04                 &  11.75             &  16.96             &  12.78                  &  11.18              &  14.92             &  20                    \\
    LSTM              &  13.27                 &  10.81             &  13.65             &  11.91                  &  10.34              &  13.4              &  NA                    \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}[th]
  \caption{Transfer learning results}
  \label{tab:trans}
  \centering
  \begin{tabular}{ lllllll }
    \toprule
    CER               &  dev\_android           &  dev\_ios           &  dev\_mic           &  test\_android           &  test\_ios           &  test\_mic     \\
    \midrule
    Baseline          &  15.04                 &  11.75             &  16.96             &  12.78                  &  11.18              &  14.92        \\
    Channel transfer  &  12.88                 &  12.16             &  12.13             &  11.65                  &  11.48              &  10.79        \\
    Domain transfer   &  8.07                  &  6.65              &  9.16              &  6.65                   &  5.63               &  8.05         \\
    Combined transfer &  6.66                  &  6.43              &  6.21              &  5.97                   &  5.77               &  5.48         \\
    \bottomrule
  \end{tabular}
\end{table*}

\section{Customizing a speech recognition system}

One major barrier that stops the transforming from research prototypes into
industrial applications is the scale of training data. Now that AISHELL-2 corpus
and baseline system have been released, the potential of transforming the
Mandarin ASR research into industrial scale can be fully discovered by
customizing a speech recognition system based on the AISHELL-2
baseline. Customizing a speech recognition system requires transferring the
knowledge of a baseline system into a specific scenario which suits the target
application. The knowledge include the channel through which the speech data is
collected, and the language domain context of target applications.

\subsection{Acoustic channel fine-tuning}

Table~\ref{tab:base} shows that channel mismatch introduces significant
performance decay. To alleviate this type of problem, various adaptation and
transfer learning techniques have been proposed, depending on the amount of
available matched data~\cite{adapt}. In AISHELL-2, we experiment the most
straight forward method - acoustic model finetuning. A finetune recipe is
provided, which takes matched data and mismatched model as input, and produces
adapted model that suits the target channel. In the experiment, 100 hours of
data from the mic channel is forced-aligned and fed into the baseline TDNN model
(trained with ios data) via standard back-propagation, and results are shown in
Table~\ref{tab:trans}. By transferring the channel of acoustic model, the
performance on the matched subset, a.k.a. the mic subset, is significantly
improved. Meanwhile, there is slight regression of the performance on the
android subset. The performance on the android subset is also improved, which
might due to better robustness of the acoustic model.

\subsection{Language domain adaptation}

The domain of language model is also crucial for applicational ASR
systems. LM training text of AISHELL-2 are mostly collected from smart-home
domain. In order to demonstrate domain transfer, we delibrately designed dev and
test sets from general domain, then we collected 3 million sentences in addition
(approximately 100 megabytes of text) from same domain as dev and test sets. The
texts are segmented using the method as in Section 3.1. A trigram is estimated
using modified Kneser-Ney smoothing~\cite{kn} and interpolated with the
baseline LM using a weight of 0.5. During evaluation, the baseline AM is coupled
with this interpolated LM. Table~\ref{tab:trans} shows that, by applying domain
transfer, CER is decreased by approximately 45\% on all the subsets.

Furthermore, we combined the finetuned AM with the interpolated LM. The results
are shown in the last row of Table~\ref{tab:trans}. The CER of all subsets for
all channels are further decreased to around 6\%, suggesting that the robustness
and performance of the baseline system is significantly improved via simple
acoustic and language model transfer learning.


\subsection{Real-time demo system}

Unlike most of the other open-source ASR recipes, AISHELL-2 provides a real-time
demo so that developers can actually try their ASR systems via laptop
microphone. We used PortAudio to capture laptop microphone and stream speech
data into Kaldi's online decoder in real-time, partial and final recognition
results are flushed periodically on laptop screen, like a real-time commercial
speech transcriber.

Although acoustic channel mismatch (between AM and laptop mic) may degrade the
ASR performance, the point is that we extend recipe's evaluation approach beyond
sole WER numbers. We believe this end-to-end experience may help researchers and
industry developers get a first-hand idea on how well their models perform.

\section{Conclusions}

In this paper, AISHELL-2 Mandarin corpus and AISHELL-2 recipes are released,
freely available to research community, for building industrial scale research
baselines. We hope this open-source project may provide essential gradients for
researchers to explore more scalable techniques regarding to industrial
issues. To customize a research baseline into industrial application, we discuss
and experiment different approaches to deal with system robustness, acoustic
channel mismatch and application domain mismatch. Further more, by developing
a real-time ASR demo program, we establish a bridge between Character-Error-Rate
evaluation and first-hand experience via laptop microphone.

\section{Acknowledgements}

The authors thank all members of AISHELL foundation who contributed to this
project and Emotech Labs who provided computational resources for producing most recent system performance statistics.

\bibliographystyle{IEEEtran}

\bibliography{mybib}

% \begin{thebibliography}{9}
% \bibitem[1]{Davis80-COP}
%   S.\ B.\ Davis and P.\ Mermelstein,
%   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% \bibitem[2]{Rabiner89-ATO}
%   L.\ R.\ Rabiner,
%   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% \bibitem[3]{Hastie09-TEO}
%   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%   New York: Springer, 2009.
% \bibitem[4]{YourName17-XXX}
%   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%   ``Title of your INTERSPEECH 2018 publication,''
%   in \textit{Interspeech 2018 -- 19\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 2-6, Hyderabad, India Proceedings, Proceedings}, 2018, pp.~100--104.
% \end{thebibliography}

\end{document}
