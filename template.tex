\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2018}
\usepackage{subfigure}

\title{AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale}
\name{Jiayu Du$^1$, Xingyu Na$^1$, Xuechen Liu$^1$, Hui Bu$^2$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$AISHELL foundation$^*$\thanks{* AISHELL foundation is a non-profit online organization, dedicated to pushing forward speech industry via open-sourcing database to research institutes and contributing codes to open-source speech community.} \\
  $^2$Beijing Shell Shell Technology Co. Ltd., Beijing, China}
\email{aishell.foundation@gmail.com, buhui@aishelldata.com}

\begin{document}

\maketitle
%
\begin{abstract}
AISHELL-1 is by far the largest open-source speech corpus available for Mandarin
speech recognition research. It was released with a baseline system containing solid
training and testing pipelines for Mandarin ASR. In AISHELL-2, 1000 hours of
clean read-speech data from iOS is published, which is free for academic usage. On top of
AISHELL-2 corpus, an improved recipe is developed and released, containing key
components for industrial applications, such as Chinese word segmentation,
flexible vocabulary expension and phone set transformation etc. 
Pipelines support various state-of-the-art techniques, such
as time-delayed neural networks and Lattic-Free MMI objective funciton. 
In addition, we also release dev and test data from other channels(Android and Mic). 
For research community, we hope that AISHELL-2
corpus can be a solid resource for topics like transfer learning and robust
ASR. For industry, we hope AISHELL-2 recipe can be a helpful reference for
building meaningful industrial systems and products.
\end{abstract}
\noindent\textbf{Index Terms}: Speech recognition, Mandarin ASR, Industrial Speech Recognition

\section{Introduction}

Automatic Speech Recognition (ASR) is one of the application domains in the bloom of Artificial
Intelligence (AI) which recently received huge attention. Huge effort has been made from both research community and industry
to improve ASR system performance. Among all solutions proposed, ones based on Deep Learning and its generations has been dominating
in the recent half decade. One advantage offered by it is that neural network (NN) models generally turn out to be more robust when
huge amount of data is provided. This is good since thanks to the emerging market of various smart devices with rising online network usage, data from
real-time scenarios can sometimes be fetched consistently. Therefore, accessing and 
collecting data in a proper fashion has become easier than before, especially from industrial perpective.
However, on the other hand, research community still normally has limited access to real-world
application data. Most ASR solutions, as a result, can not scale well in real-time practice and hence not applicable for industrial use. This leads to the need of open-sourced high-quality corpus for mostly research use while being easily deployable to industrial level, just like what happened in the field of computer vision (e.g. ImageNet~\cite{imagenet} and COCO~\cite{coco}). Although there are corpus like thchs30~\cite{thchs30} and hkust~\cite{hkust1}, no such corpus in Mandarin ASR which can meets all those requirements above has been released until AISHELL-1~\cite{aishell1}.
%In the field of computer vision, there are many high quality free data sets which transform many research
%efforts into industrial applications, such as ImageNet~\cite{imagenet} and
%COCO~\cite{coco}. However, in the field of Mandarin ASR, there was no free
%corpus with sufficient amount of high quality speech data to build a meaningful
%research baseline, until the release of AISHELL-1~\cite{aishell1}.
It contains 170 hours of Mandarin speech with high quality human transcriptions. 
Various training and evaluation recipes based on such corpus have been developed and released in Kaldi~\cite{kaldi}, 
which is a robust and widely-acknowledged framework for speech research. 
To the best knowledge of the authors, it is the first fully open-sourced system for Mandarin ASR with 
high quality Mandarin speech data(e.g. ~\cite{do2017, do2018_1, do2018_2}).

% Another issue with the Mandarin speech recognition research is that, there is
% still gap between a research prototype for demonstration and a working system
% for application. The open-source movement improves the transparency and
% availability of many research achievements. However, the attemp of direct
% deployment of a working system from an open-source demostration often fail. The
% effort of knowledage transfering from the research topic to application domain
% is enourmously high, and is normally off the record.

Furthering the success from AISHELL-1 with efforts, in this paper, we introduce AISHELL-2, as an open-sourced while self-contained baseline for industrial-scale Mandarin ASR research. 
Several recipes along with the corpus has been developed, containing must-have components such as Chinese word segmentation, 
customizable Chinese lexicon etc, following what have been done with AISHELL-1 formally.
Moreover, following the training data, development and test data from 3 different acoustic channels(iOS, Android and Mic) are released and open-sourced as well.
We hope these resources would be helpful for Mandarin ASR research, making it get closer with industrial-scale level.

The rest of this paper is organized as following: The details of AISHELL-2 corpus data is introduced in Section 2. Section 3 describes the Mandarin ASR pipeline for producing baseline system and Section 4 presents system performance on test sets from different acoustic channels.

\section{AISHELL-2 corpus}

AISHELL-2 corpus contains 1000 hours of clean read-speech data, which will be released to academic research community for free. Raw data was recorded via three parallel acoustic channels - a high fidelity microphone (Mic), an Android smartphone (Android) and an iPhone (iOS). The relative position between speaker and devices are shown in Figure~\ref{fig:setup}. Data from iPhone, i.e. the iOS channel is open-sourced. Speaker, environment and content coverage are explained as below:
\begin{itemize}
\item \textbf{Speaker information.} There are 1991 speakers participated in the recording, including 845 male and 1146 female. Age of speaker ranges from 11 to over 40. Ideally the speakers shall speak everything to be recorded in Mandarin, while there are some slight accent variations. Generally speaking of the accents, there were 1293 speakers using Northern ones, 678 speakers using Southern ones and 20 speakers use other accents during recording. 
\item \textbf{Recording environment.} 1347 speakers are recorded in a studio, while the rest are in a living room with natural reverberation.
\item \textbf{Content of speech}. The content of the recording covers 8 major topics: voice commands such as IoT device control and digital sequential input, places of interest, entertainment, finance, technology, sports, English spellings and free speaking without specific topic. The total number of prompts is around half a million.
\end{itemize}

\noindent Aside from AISHELL-2 corpus introduced above, which is supposed to be used for training, 
we also provide development and test sets. 
Development set contains 2500 utterances from 5 speakers and test set contains 5000 utterances from 10 speakers. 
Each speaker contributed approximately half an hour of speech, covering 500 prompts. 
First 7 prompts of all speakers are extracted from high frequency online queries. Speaker-gender is balanced as well.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{setup.jpg}
  \caption{Recording setup}
  \label{fig:setup}
\end{figure}

\section{AISHELL-2 recipe}

As briefly mentioned in Section 1, based on AISHELL-2 corpus, several recipes are released to Kaldi repository as a complete collection, including data and lexicon preparation, language model training, Gaussian mixture model (GMM) and Neural Network training, as well as test set evaluation procedure.

\subsection{Lexicon and word segmentation}

Unlike English ASR system, Mandarin ASR often requires sophisticated word segmentation. 
In AISHELL-1, word segmentation was implemented via forward
maximum matching algorithm \textbf{[Ref?]}, based on a variation of open-source mandarin
dictionary CC-CEDIT\footnote{https://cc-cedict.org/wiki}. In AISHELL-2, an
open-source Chinese dictionary called DaCiDian is
released\footnote{https://github.com/aishell-foundation/DaCiDian}. In most
common Chinese dictionaries, words are directly mapped to phonemes. While in
DaCiDian, this mapping is decomposed into 2 independent layers. An exemplar
DaCiDian structure is shown in Figure~\ref{fig:lex1} and Figure~\ref{fig:lex2}. The first layer maps word to PinYin syllables~\cite{pinyin}. Anyone who is
  familiar with PinYin (basically every Mandarin speaker) can enrich DaCiDian's
  vocabulary by adding new words into this layer. The second layer is a mapping from Pinyin syllable to phoneme. ASR system
  developers can easily adapt DaCiDian to their own phone set by redefining this
  layer of mapping.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{dacidianl1.png}
  \caption{Layer 1 of DaCiDian}
  \label{fig:lex1}
\end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{dacidianl2.png}
  \caption{Layer 2 of DaCiDian}
  \label{fig:lex2}
\end{figure}

In terms of word segmentation, we choosed a popular and easy-to-use open-source
toolkit called Jieba~\cite{jieba}, it implemented a trie-tree based algorithm and
supports vocabulary customization. Based on DaCiDian and Jieba, we provide a
script to segment AISHELL-2 transcription and language model text.

\subsection{Acoustic model}

Acoustic training contains two stages: GMM-HMM state model training based on maximum likelihood and later the training of a hybrid DNN-HMM state estimator. Both can be implemented by calling standard methods and designing corresponding recipes under the framework of Kaldi.

The GMM models were firstly trained using 13 dimensional MFCC plus pitches, which made the input dimension 16.
A monophone model was trained to set a starting point for the triphone models. A small triphone model and a larger triphone model were then consecutively trained using delta features. After that, a more sophisticated feature transform method was applied to replace the delta features. Linear discriminant analysis (LDA) was applied on stack of frames to reduce the dimension and MLLT-based global transform is estimated iteratively. This follows a standard setup pipeline in the majority of available Kaldi recipes. The resulting number of physical GMM states from the four steps were 605, 3216, 5720 and 8080 respectively.

GMM training of the AISHELL-2 stopped at the speaker independent stage without speaker dependent transform involved, such as fMLLR. For industrial-scale corpus, it is not worth spending too much time and computation power at GMM-HMM training stage, since the final system performance primarily depends on later neural network models. Therefore, we adopted speaker independent GMM training and pushed the speaker dependent steps to later DNN training phase, as described below.

Based on the tri-state alignments provided by GMM stages above, a time-delayed neural network (TDNN, \cite{tdnn}) is then configured and trained. It has 8 layers in total with 1280 hidden units in each layer. It is, as described in \cite{tdnn}, not fully connected - the input of each hidden layer is a frame-wise spliced output of its preceding layer. The input feature was high resolution MFCC with cepstral normalization plus pitches, which made its dimension 43. Note that for each frame-wise input a 100-dimensional i-vector~\cite{ivector} was also attached, whose extractor was trained based on the corpus itself. The corresponding diagonal universal background model (UBM) was trained using a quarter of training features. This indicates that different from GMM-level training, we encoded speaker information here to produce a stronger baseline for research community. The network was trained using lattice-free maximum mutual information (LFMMI, \cite{lfmmi}) as the objective function. More configurations about NN training itself such as lattice generation and tree topology can be found at \cite{lfmmi} and AISHELL-2 exemplar scripts in Kaldi repository \footnote{https://github.com/kaldi-asr/kaldi/egs/aishell2}.

\subsection{Language model}

A trigram language model was trained on a 5.7 million-word database. Out-of-vocabulary (OOV) words were mapped as $<$UNK$>$. The 3-order ARPA language model is trained using Kneser-Ney smoothing. The final model had 516552 unigrams, 1498603 bigrams and 932475 trigrams.

\section{Experiment and evaluations}

Speaking of hardware, the presented baseline system was trained on a standalone server, with 36 cores of Intel Xeon (2.3GHz) for all cpu-based steps and 4
Tesla K80 processors for DNN model training. Character Error Rate (CER) was used as the evaluation metric. 

Results along with the total training time of each stage, which was counted in unit of hours, are presented in Table~\ref{tab:base}. 
Note that as to keep consistency with future community usage, during training we only used open-sourced iOS data from the beginning to the end while evaluated these models on dev and test sets for all 3 channels (Android, Mic, and iOS). System performance on iOS outperformed Android and Mic, which is as expected due to better acoustic channel condition matching.

\begin{table*}[th]
  \caption{Baseline system results and training time}
  \label{tab:base}
  \centering
  \begin{tabular}{ llllllll }
    \toprule
    CER               &  dev\_android           &  dev\_ios           &  dev\_mic           & test\_android            &  test\_ios           &  test\_mic          &  Training time         \\
    \midrule
    Mono        &  47.08                 &  43.37             &  47.33             &  45.40                  &  44.81              &  44.28             &  0.5                   \\
    tri1    &  26.61                 &  22.94             &  26.55             &  26.08                  &  24.79              &  25.36             &  1                     \\
    tri2    &  24.59                 &  21.47             &  24.59             &  23.82                 &  22.69              &  23.37             &  2                     \\
    tri3(LDA+MLLT)          &  22.24                 &  18.86             &  22.47             &  21.00                  &  19.77              &  21.10             &  2.5                   \\
    Chain-TDNN              &  10.58                 &  9.01             &  12.37         &  9.68                  &  8.82             &  11.28              &  15                    \\
    \bottomrule
  \end{tabular}
\end{table*}

% \begin{table*}[th]
%  \caption{Transfer learning results}
%  \label{tab:trans}
%  \centering
%  \begin{tabular}{ lllllll }
%    \toprule
%    CER               &  dev\_android           &  dev\_ios           &  dev\_mic           &  test\_android           &  test\_ios           &  test\_mic     \\
%    \midrule
%    Baseline          &  15.04                 &  11.75             &  16.96             &  12.78                  &  11.18              &  14.92        \\
%    Channel transfer  &  12.88                 &  12.16             &  12.13             &  11.65                  &  11.48              &  10.79        \\
%    Domain transfer   &  8.07                  &  6.65              &  9.16              &  6.65                   &  5.63               &  8.05         \\
%    Combined transfer &  6.66                  &  6.43              &  6.21              &  5.97                   &  5.77               &  5.48         \\
%    \bottomrule
%  \end{tabular}
% \end{table*}

% \section{Customizing a speech recognition system}
% 
% One major barrier that stops the transforming from research prototypes into
% industrial applications is the scale of training data. Now that AISHELL-2 corpus
% and baseline system have been released, the potential of transforming the
% Mandarin ASR research into industrial scale can be fully discovered by
% customizing a speech recognition system based on the AISHELL-2
% baseline. Customizing a speech recognition system requires transferring the
% knowledge of a baseline system into a specific scenario which suits the target
% application. The knowledge include the channel through which the speech data is
% collected, and the language domain context of target applications.
% 
% \subsection{Acoustic channel fine-tuning}
% 
% Table~\ref{tab:base} shows that channel mismatch introduces significant
% performance decay. To alleviate this type of problem, various adaptation and
% transfer learning techniques have been proposed, depending on the amount of
% available matched data~\cite{adapt}. In AISHELL-2, we experiment the most
% straight forward method - acoustic model finetuning. A finetune recipe is
% provided, which takes matched data and mismatched model as input, and produces
% adapted model that suits the target channel. In the experiment, 100 hours of
% data from the mic channel is forced-aligned and fed into the baseline TDNN model
% (trained with ios data) via standard back-propagation, and results are shown in
% Table~\ref{tab:trans}. By transferring the channel of acoustic model, the
% performance on the matched subset, a.k.a. the mic subset, is significantly
% improved. Meanwhile, there is slight regression of the performance on the
% android subset. The performance on the android subset is also improved, which
% might due to better robustness of the acoustic model.
% 
% \subsection{Language domain adaptation}
% 
% The domain of language model is also crucial for applicational ASR
% systems. LM training text of AISHELL-2 are mostly collected from smart-home
% domain. In order to demonstrate domain transfer, we delibrately designed dev and
% test sets from general domain, then we collected 3 million sentences in addition
% (approximately 100 megabytes of text) from same domain as dev and test sets. The
% texts are segmented using the method as in Section 3.1. A trigram is estimated
% using modified Kneser-Ney smoothing~\cite{kn} and interpolated with the
% baseline LM using a weight of 0.5. During evaluation, the baseline AM is coupled
% with this interpolated LM. Table~\ref{tab:trans} shows that, by applying domain
% transfer, CER is decreased by approximately 45\% on all the subsets.
% 
% Furthermore, we combined the finetuned AM with the interpolated LM. The results
% are shown in the last row of Table~\ref{tab:trans}. The CER of all subsets for
% all channels are further decreased to around 6\%, suggesting that the robustness
% and performance of the baseline system is significantly improved via simple
% acoustic and language model transfer learning.

\section{Conclusions}

In this paper, we generally introduce AISHELL-2, a 1000 hour Mandarin ASR corpus which is freely available to research community for building industrial scale speech solutions, as well as its following utilities and some details about the pipeline for baseline results. We hope this open-source project is able to provide essential ingredients for researchers to explore more scalable and practical solutions regarding to industrial issues in the field of Mandarin speech recognition and will keep improving it from every aspect.

\section{Acknowledgements}

The authors would like to thank all other members of AISHELL foundation who contributed to this project and Emotech Labs who provided computational resources for producing most recent system performance statistics.

\bibliographystyle{IEEEtran}

\bibliography{mybib}

% \begin{thebibliography}{9}
% \bibitem[1]{Davis80-COP}
%   S.\ B.\ Davis and P.\ Mermelstein,
%   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% \bibitem[2]{Rabiner89-ATO}
%   L.\ R.\ Rabiner,
%   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% \bibitem[3]{Hastie09-TEO}
%   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%   New York: Springer, 2009.
% \bibitem[4]{YourName17-XXX}
%   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%   ``Title of your INTERSPEECH 2018 publication,''
%   in \textit{Interspeech 2018 -- 19\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 2-6, Hyderabad, India Proceedings, Proceedings}, 2018, pp.~100--104.
% \end{thebibliography}

\end{document}
